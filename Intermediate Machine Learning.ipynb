{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Kaggle: Intermediate Machine Learning </b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy dataframe and train/valuation segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"file_name.csv\")\n",
    "y = df.\"predictor_col_name\"      \n",
    "exp_rows = [\"col_name1\", \"col_name2\", ...]\n",
    "X = df[exp_rows]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Chapter 2: Missing Values </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to missing values:\n",
    "1. Drop columns\n",
    "2. Imputation (Replace with values: mean, median etc...)\n",
    "3. Imputation with new binary column suggesting missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function returning error size of models\n",
    "def score_dataset(train_X, val_X, train_y, val_y):\n",
    "    model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "    return mean_absolute_error(val_y, preds) # Can replace MAE with other types of error metric\n",
    "\n",
    "# 1. Dropping columns\n",
    "cols_with_missing = [col for col in train_X.columns if train_X[col].isnull().any()]\n",
    "reduced_train_X = train_X.drop(cols_with_missing, axis = 1)\n",
    "reduced_val_X = val_X.drop(cols_with_missing, axis = 1)\n",
    "\n",
    "# 2. Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_train_X = pd.DataFrame(my_imputer.fit_transform(train_X))\n",
    "imputed_val_X = pd.DataFrame(my_imputer.transform(val_X))\n",
    "imputed_train_X.columns = train_X.columns    # Put back column names\n",
    "imputed_val_X.columns = val_X.columns\n",
    "\n",
    "# 3. Imputation with dummy missing column\n",
    "train_X_plus = train_X.copy()\n",
    "val_X_plus = val_X.copy()\n",
    "for col in cols_with_missing:                # For making new dummy columns\n",
    "    train_X_plus[col + '_was_missing'] = train_X_plus[col].isnull()\n",
    "    val_X_plus[col + '_was_missing'] = val_X_plus[col].isnull()\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_train_X_plus = pd.DataFrame(my_imputer.fit_transform(train_X_plus))\n",
    "imputed_val_X_plus = pd.DataFrame(my_imputer.transform(val_X_plus))\n",
    "imputed_train_X_plus.columns = train_X_plus.columns\n",
    "imputed_val_X_plus.columns = val_X_plus.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Chapter 3: Categorical Variables </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to categorical variables:\n",
    "1. Drop variables\n",
    "2. Ordinal encoding\n",
    "3. One-Hot encoding (n binary cols for n categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Get list of categorical variables\n",
    "object_cols = [col for col in train_X.columns if train_X[col].dtype == \"object\"]\n",
    "\n",
    "# 1. Drop categorical variables\n",
    "drop_train_X = train_X.select_dtypes(exclude = ['object'])\n",
    "drop_val_X = val_X.select_dtypes(exclude = ['object'])\n",
    "\n",
    "# 2. Ordinal encoding \n",
    "label_train_X = train_X.copy()\n",
    "label_val_X = val_X.copy()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Ordinal encoding will output error if column names if train data doesnt equal validation data\n",
    "# Only use repeated columns in both train and validation set\n",
    "good_label_cols = [col for col in object_cols if set(val_X[col]).issubset(set(train_X[col]))]\n",
    "bad_label_cols = list(set(object_cols) - set(good_label_cols))  # Problematic columns, will be dropped\n",
    "\n",
    "label_train_X = train_X.drop(bad_label_cols, axis=1)            # Drop problematic columns\n",
    "label_val_X = val_X.drop(bad_label_cols, axis=1)\n",
    "label_train_X[good_label_cols] = ordinal_encoder.fit_transform(train_X[good_label_cols])\n",
    "label_val_X[good_label_cols] = ordinal_encoder.transform(val_X[good_label_cols])\n",
    "\n",
    "# 3. One-Hot encoding\n",
    "# Only choose columns with low cardinality (low # of categories), or else size of dataset explodes\n",
    "low_cardinality_cols = [col for col in object_cols if train_X[col].nunique() < 10]\n",
    "high_cardinality_cols = list(set(object_cols) - set(low_cardinality_cols))\n",
    "\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(val_X[low_cardinality_cols]))\n",
    "\n",
    "OH_cols_train.index = train_X.index                             # Put back column names\n",
    "OH_cols_valid.index = val_X.index\n",
    "\n",
    "num_train_X = train_X.drop(object_cols, axis = 1)               # Remove categorical columns\n",
    "num_val_X = val_X.drop(object_cols, axis = 1)\n",
    "\n",
    "OH_train_X = pd.concat([num_train_X, OH_cols_train], axis = 1)  # Add one-hot encoded columns\n",
    "OH_val_X = pd.concat([num_val_X, OH_cols_valid], axis = 1)\n",
    "\n",
    "OH_train_X.columns = OH_train_X.columns.astype(str)             # Ensure all columns are string dtype\n",
    "OH_val_X.columns = OH_val_X.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Chapter 4: Pipelines </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Check for missing values\n",
    "cols_with_missing = [col for col in train_X.columns if train_X[col].isnull().any()]\n",
    "\n",
    "# 2. Check for categorical variables\n",
    "object_cols = [col for col in train_X.columns if train_X[col].dtype == \"object\"]\n",
    "\n",
    "# 3. Preprocess numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# 4. Preprocess categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# 5. Initiate preprocessor for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# 6. Initiate model\n",
    "model = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "\n",
    "# 7. Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# 8. Fit model \n",
    "my_pipeline.fit(train_X, train_y)\n",
    "\n",
    "# 9. Predict y\n",
    "preds = my_pipeline.predict(val_X)\n",
    "\n",
    "# 10. Evaluate model\n",
    "score = mean_absolute_error(val_y, preds)\n",
    "\n",
    "# 11. Publish model on full data X = train_X + val_X\n",
    "preds_test = my_pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Chapter 5: Cross-Validation </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point of cross-validation: Set different chunks of data to be validation and training, so to generate more hollistic error values for evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Chapter 6: XGBoost </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting methodology:\n",
    "1. Use single model for prediction (Naive model)\n",
    "2. Calculate loss function (MSE, MAE, etc...)\n",
    "3. Fit new model to reduce loss\n",
    "4. Add new model to ensemble\n",
    "5. Predict and repeat 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Gradient boosting regressor model\n",
    "model = XGBRegressor(n_estimators = ..., learning_rate = ..., n_jobs = ...)\n",
    "model.fit(train_X, train_y, early_stopping_rounds = ...,\n",
    "          eval_set = [(val_X, val_y)], verbose = False)\n",
    "predictions = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor parameters:\n",
    "1. n_estimators: Too little params = underfit, too many will overfit; Typically 100-1000\n",
    "2. early_stopping_rounds: Stops model from iterating when loss function stops improving; Typically 5\n",
    "3. learning_rate: Weighting on new models; Default 0.1\n",
    "4. n_jobs: Number of model fitting simultaneously; Typical set as # of cores on machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Chapter 7: Data Leakage </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
